{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-28T08:33:40.923608Z",
     "start_time": "2025-01-28T07:50:55.942092Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import gym\n",
    "\n",
    "env = gym.make(\"MountainCarContinuous-v0\", render_mode=\"human\", max_episode_steps=1000)\n",
    "state_space = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "\n",
    "\n",
    "\n",
    "def PolicyModel(input_shape, action_dim):\n",
    "    inp = Input(shape=(input_shape,))\n",
    "    x = Dense(64, activation='relu')(inp)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    # mean and log_std for the action dimension\n",
    "    mean = Dense(action_dim, activation='tanh')(x)\n",
    "    log_std = Dense(action_dim, activation='linear')(x)\n",
    "    model = Model(inputs=inp, outputs=[mean, log_std])\n",
    "    return model\n",
    "\n",
    "policy_model = PolicyModel(state_space, action_dim)\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "\n",
    "@tf.function\n",
    "def train(cu_rewards, states, actions):\n",
    "    cu_rewards = (cu_rewards - tf.reduce_mean(cu_rewards))/(tf.math.reduce_std(cu_rewards)+1e-8)\n",
    "    cum_rewards = tf.cast(cu_rewards, tf.float32)\n",
    "    states = tf.convert_to_tensor(states, dtype=tf.float32)\n",
    "    actions = tf.convert_to_tensor(actions, dtype=tf.float32)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        means, log_stds = policy_model(states)\n",
    "        log_stds = tf.clip_by_value(log_stds, -5.0, 2)\n",
    "        stds = tf.exp(log_stds)\n",
    "        log_action_proba = -0.5 * ((actions - means) / stds)**2 - log_stds - 0.5 * tf.math.log(2 * np.pi)\n",
    "        loss = -tf.reduce_mean(log_action_proba*cum_rewards)\n",
    "    gradients = tape.gradient(loss, policy_model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, policy_model.trainable_variables))\n",
    "    print(\"Loss: \", loss)\n",
    "\n",
    "\n",
    "\n",
    "def calc_cumulative_rewards(rewards, gamma):\n",
    "    cumulative_rewards = np.zeros_like(rewards)\n",
    "    for i in reversed(range(len(rewards))):\n",
    "        if i == (len(rewards) - 1):\n",
    "            cumulative_rewards[i] = rewards[i]\n",
    "        else:\n",
    "            cumulative_rewards[i] = rewards[i] + gamma * cumulative_rewards[i + 1]\n",
    "\n",
    "    return cumulative_rewards\n",
    "\n",
    "\n",
    "\n",
    "gamma = 0.99\n",
    "\n",
    "episodes = 200\n",
    "experiences= []\n",
    "step = 0\n",
    "for j in range(episodes):\n",
    "    steps_per_episode = 0\n",
    "    state = env.reset()[0]\n",
    "    actions = []\n",
    "    states = []\n",
    "    rewards = []\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        state_input = np.expand_dims(state, axis=0)\n",
    "\n",
    "        mean, log_std = policy_model.predict(state_input, verbose=0)\n",
    "        mean, log_std = mean[0], log_std[0]\n",
    "\n",
    "        log_std = np.clip(log_std, -5, 2)\n",
    "        std = np.exp(log_std)\n",
    "\n",
    "        action = np.random.normal(mean, std)\n",
    "        action = np.clip(action, env.action_space.low, env.action_space.high)\n",
    "\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "        reward = reward - 0.1\n",
    "        if done:\n",
    "            reward += 200\n",
    "\n",
    "        actions.append(action)\n",
    "        states.append(state)\n",
    "        rewards.append(reward)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        steps_per_episode += 1\n",
    "        step += 1\n",
    "        if step % 300 == 0:\n",
    "            cu_rewards = calc_cumulative_rewards(rewards, gamma)\n",
    "            train(cu_rewards, states, actions)\n",
    "\n",
    "    cu_rewards = calc_cumulative_rewards(rewards, gamma)\n",
    "    experiences.append([cu_rewards, states, actions])\n",
    "    for experience in experiences:\n",
    "        train(experience[0], experience[1], experience[2])\n",
    "    print(f\"Total Reward for episode {j}: \", total_reward)\n",
    "env.close()\n",
    "\n",
    "policy_model.save(\"continuous_mountain_car_model.keras\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function train at 0x00000226729D9480> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function train at 0x00000226729D9480> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Total Reward for episode 0:  -1647.8619076164014\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Total Reward for episode 1:  -482.5059659032331\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Total Reward for episode 2:  -350.46534913413717\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n",
      "Loss:  Tensor(\"Neg:0\", shape=(), dtype=float32)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 77\u001B[0m\n\u001B[0;32m     74\u001B[0m state_input \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mexpand_dims(state, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m     76\u001B[0m \u001B[38;5;66;03m# Predict mean and log_std\u001B[39;00m\n\u001B[1;32m---> 77\u001B[0m mean, log_std \u001B[38;5;241m=\u001B[39m \u001B[43mpolicy_model\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate_input\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     78\u001B[0m mean, log_std \u001B[38;5;241m=\u001B[39m mean[\u001B[38;5;241m0\u001B[39m], log_std[\u001B[38;5;241m0\u001B[39m]  \u001B[38;5;66;03m# Remove batch dimension\u001B[39;00m\n\u001B[0;32m     80\u001B[0m \u001B[38;5;66;03m# Convert log_std to std\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Reinforcement_Learning_using_OpenAI_Gym\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    115\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    116\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 117\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    118\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    119\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32m~\\PycharmProjects\\Reinforcement_Learning_using_OpenAI_Gym\\.venv\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:497\u001B[0m, in \u001B[0;36mTensorFlowTrainer.predict\u001B[1;34m(self, x, batch_size, verbose, steps, callbacks)\u001B[0m\n\u001B[0;32m    492\u001B[0m \u001B[38;5;129m@traceback_utils\u001B[39m\u001B[38;5;241m.\u001B[39mfilter_traceback\n\u001B[0;32m    493\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpredict\u001B[39m(\n\u001B[0;32m    494\u001B[0m     \u001B[38;5;28mself\u001B[39m, x, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauto\u001B[39m\u001B[38;5;124m\"\u001B[39m, steps\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, callbacks\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    495\u001B[0m ):\n\u001B[0;32m    496\u001B[0m     \u001B[38;5;66;03m# Create an iterator that yields batches of input data.\u001B[39;00m\n\u001B[1;32m--> 497\u001B[0m     epoch_iterator \u001B[38;5;241m=\u001B[39m \u001B[43mTFEpochIterator\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    498\u001B[0m \u001B[43m        \u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    499\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    500\u001B[0m \u001B[43m        \u001B[49m\u001B[43msteps_per_epoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msteps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    501\u001B[0m \u001B[43m        \u001B[49m\u001B[43mshuffle\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    502\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdistribute_strategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdistribute_strategy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    503\u001B[0m \u001B[43m        \u001B[49m\u001B[43msteps_per_execution\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msteps_per_execution\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    504\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    506\u001B[0m     \u001B[38;5;66;03m# Container that configures and calls callbacks.\u001B[39;00m\n\u001B[0;32m    507\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(callbacks, callbacks_module\u001B[38;5;241m.\u001B[39mCallbackList):\n",
      "File \u001B[1;32m~\\PycharmProjects\\Reinforcement_Learning_using_OpenAI_Gym\\.venv\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:715\u001B[0m, in \u001B[0;36mTFEpochIterator.__init__\u001B[1;34m(self, distribute_strategy, *args, **kwargs)\u001B[0m\n\u001B[0;32m    713\u001B[0m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    714\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_distribute_strategy \u001B[38;5;241m=\u001B[39m distribute_strategy\n\u001B[1;32m--> 715\u001B[0m dataset \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata_adapter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_tf_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    716\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(dataset, tf\u001B[38;5;241m.\u001B[39mdistribute\u001B[38;5;241m.\u001B[39mDistributedDataset):\n\u001B[0;32m    717\u001B[0m     dataset \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_distribute_strategy\u001B[38;5;241m.\u001B[39mexperimental_distribute_dataset(\n\u001B[0;32m    718\u001B[0m         dataset\n\u001B[0;32m    719\u001B[0m     )\n",
      "File \u001B[1;32m~\\PycharmProjects\\Reinforcement_Learning_using_OpenAI_Gym\\.venv\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\array_data_adapter.py:231\u001B[0m, in \u001B[0;36mArrayDataAdapter.get_tf_dataset\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    228\u001B[0m     dataset \u001B[38;5;241m=\u001B[39m dataset\u001B[38;5;241m.\u001B[39mwith_options(options)\n\u001B[0;32m    229\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m dataset\n\u001B[1;32m--> 231\u001B[0m indices_dataset \u001B[38;5;241m=\u001B[39m \u001B[43mindices_dataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mflat_map\u001B[49m\u001B[43m(\u001B[49m\u001B[43mslice_batch_indices\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    232\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m shuffle \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbatch\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    233\u001B[0m     indices_dataset \u001B[38;5;241m=\u001B[39m indices_dataset\u001B[38;5;241m.\u001B[39mmap(tf\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mshuffle)\n",
      "File \u001B[1;32m~\\PycharmProjects\\Reinforcement_Learning_using_OpenAI_Gym\\.venv\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:2389\u001B[0m, in \u001B[0;36mDatasetV2.flat_map\u001B[1;34m(self, map_func, name)\u001B[0m\n\u001B[0;32m   2385\u001B[0m \u001B[38;5;66;03m# Loaded lazily due to a circular dependency (dataset_ops -> flat_map_op ->\u001B[39;00m\n\u001B[0;32m   2386\u001B[0m \u001B[38;5;66;03m# dataset_ops).\u001B[39;00m\n\u001B[0;32m   2387\u001B[0m \u001B[38;5;66;03m# pylint: disable=g-import-not-at-top,protected-access\u001B[39;00m\n\u001B[0;32m   2388\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdata\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mops\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m flat_map_op\n\u001B[1;32m-> 2389\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mflat_map_op\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_flat_map\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_func\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Reinforcement_Learning_using_OpenAI_Gym\\.venv\\lib\\site-packages\\tensorflow\\python\\data\\ops\\flat_map_op.py:24\u001B[0m, in \u001B[0;36m_flat_map\u001B[1;34m(input_dataset, map_func, name)\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_flat_map\u001B[39m(input_dataset, map_func, name\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):  \u001B[38;5;66;03m# pylint: disable=unused-private-name\u001B[39;00m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"See `Dataset.flat_map()` for details.\"\"\"\u001B[39;00m\n\u001B[1;32m---> 24\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_FlatMapDataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_func\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Reinforcement_Learning_using_OpenAI_Gym\\.venv\\lib\\site-packages\\tensorflow\\python\\data\\ops\\flat_map_op.py:33\u001B[0m, in \u001B[0;36m_FlatMapDataset.__init__\u001B[1;34m(self, input_dataset, map_func, name)\u001B[0m\n\u001B[0;32m     30\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, input_dataset, map_func, name\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m     32\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_dataset \u001B[38;5;241m=\u001B[39m input_dataset\n\u001B[1;32m---> 33\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_map_func \u001B[38;5;241m=\u001B[39m \u001B[43mstructured_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mStructuredFunctionWrapper\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     34\u001B[0m \u001B[43m      \u001B[49m\u001B[43mmap_func\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_transformation_name\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_dataset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     35\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_map_func\u001B[38;5;241m.\u001B[39moutput_structure, dataset_ops\u001B[38;5;241m.\u001B[39mDatasetSpec):\n\u001B[0;32m     36\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[0;32m     37\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe `map_func` argument must return a `Dataset` object. Got \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m     38\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdataset_ops\u001B[38;5;241m.\u001B[39mget_type(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_map_func\u001B[38;5;241m.\u001B[39moutput_structure)\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\PycharmProjects\\Reinforcement_Learning_using_OpenAI_Gym\\.venv\\lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:265\u001B[0m, in \u001B[0;36mStructuredFunctionWrapper.__init__\u001B[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001B[0m\n\u001B[0;32m    258\u001B[0m       warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m    259\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mEven though the `tf.config.experimental_run_functions_eagerly` \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    260\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moption is set, this option does not apply to tf.data functions. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    261\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTo force eager execution of tf.data functions, please use \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    262\u001B[0m           \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`tf.data.experimental.enable_debug_mode()`.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    263\u001B[0m     fn_factory \u001B[38;5;241m=\u001B[39m trace_tf_function(defun_kwargs)\n\u001B[1;32m--> 265\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_function \u001B[38;5;241m=\u001B[39m \u001B[43mfn_factory\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    266\u001B[0m \u001B[38;5;66;03m# There is no graph to add in eager mode.\u001B[39;00m\n\u001B[0;32m    267\u001B[0m add_to_graph \u001B[38;5;241m&\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m context\u001B[38;5;241m.\u001B[39mexecuting_eagerly()\n",
      "File \u001B[1;32m~\\PycharmProjects\\Reinforcement_Learning_using_OpenAI_Gym\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:1251\u001B[0m, in \u001B[0;36mFunction.get_concrete_function\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1249\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_concrete_function\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m   1250\u001B[0m   \u001B[38;5;66;03m# Implements PolymorphicFunction.get_concrete_function.\u001B[39;00m\n\u001B[1;32m-> 1251\u001B[0m   concrete \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_concrete_function_garbage_collected(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1252\u001B[0m   concrete\u001B[38;5;241m.\u001B[39m_garbage_collector\u001B[38;5;241m.\u001B[39mrelease()  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[0;32m   1253\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m concrete\n",
      "File \u001B[1;32m~\\PycharmProjects\\Reinforcement_Learning_using_OpenAI_Gym\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:1221\u001B[0m, in \u001B[0;36mFunction._get_concrete_function_garbage_collected\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1219\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_variable_creation_config \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1220\u001B[0m     initializers \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m-> 1221\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_initialize\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madd_initializers_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minitializers\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1222\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_initialize_uninitialized_variables(initializers)\n\u001B[0;32m   1224\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_created_variables:\n\u001B[0;32m   1225\u001B[0m   \u001B[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001B[39;00m\n\u001B[0;32m   1226\u001B[0m   \u001B[38;5;66;03m# version which is guaranteed to never create variables.\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Reinforcement_Learning_using_OpenAI_Gym\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:696\u001B[0m, in \u001B[0;36mFunction._initialize\u001B[1;34m(self, args, kwds, add_initializers_to)\u001B[0m\n\u001B[0;32m    691\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_variable_creation_config \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate_scoped_tracing_options(\n\u001B[0;32m    692\u001B[0m     variable_capturing_scope,\n\u001B[0;32m    693\u001B[0m     tracing_compilation\u001B[38;5;241m.\u001B[39mScopeType\u001B[38;5;241m.\u001B[39mVARIABLE_CREATION,\n\u001B[0;32m    694\u001B[0m )\n\u001B[0;32m    695\u001B[0m \u001B[38;5;66;03m# Force the definition of the function for these arguments\u001B[39;00m\n\u001B[1;32m--> 696\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_concrete_variable_creation_fn \u001B[38;5;241m=\u001B[39m \u001B[43mtracing_compilation\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrace_function\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    697\u001B[0m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_variable_creation_config\u001B[49m\n\u001B[0;32m    698\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    700\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minvalid_creator_scope\u001B[39m(\u001B[38;5;241m*\u001B[39munused_args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39munused_kwds):\n\u001B[0;32m    701\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"Disables variable creation.\"\"\"\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Reinforcement_Learning_using_OpenAI_Gym\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:178\u001B[0m, in \u001B[0;36mtrace_function\u001B[1;34m(args, kwargs, tracing_options)\u001B[0m\n\u001B[0;32m    175\u001B[0m     args \u001B[38;5;241m=\u001B[39m tracing_options\u001B[38;5;241m.\u001B[39minput_signature\n\u001B[0;32m    176\u001B[0m     kwargs \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m--> 178\u001B[0m   concrete_function \u001B[38;5;241m=\u001B[39m \u001B[43m_maybe_define_function\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    179\u001B[0m \u001B[43m      \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtracing_options\u001B[49m\n\u001B[0;32m    180\u001B[0m \u001B[43m  \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    182\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m tracing_options\u001B[38;5;241m.\u001B[39mbind_graph_to_function:\n\u001B[0;32m    183\u001B[0m   concrete_function\u001B[38;5;241m.\u001B[39m_garbage_collector\u001B[38;5;241m.\u001B[39mrelease()  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\Reinforcement_Learning_using_OpenAI_Gym\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:283\u001B[0m, in \u001B[0;36m_maybe_define_function\u001B[1;34m(args, kwargs, tracing_options)\u001B[0m\n\u001B[0;32m    281\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    282\u001B[0m   target_func_type \u001B[38;5;241m=\u001B[39m lookup_func_type\n\u001B[1;32m--> 283\u001B[0m concrete_function \u001B[38;5;241m=\u001B[39m \u001B[43m_create_concrete_function\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    284\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtarget_func_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlookup_func_context\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunc_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtracing_options\u001B[49m\n\u001B[0;32m    285\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    287\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tracing_options\u001B[38;5;241m.\u001B[39mfunction_cache \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    288\u001B[0m   tracing_options\u001B[38;5;241m.\u001B[39mfunction_cache\u001B[38;5;241m.\u001B[39madd(\n\u001B[0;32m    289\u001B[0m       concrete_function, current_func_context\n\u001B[0;32m    290\u001B[0m   )\n",
      "File \u001B[1;32m~\\PycharmProjects\\Reinforcement_Learning_using_OpenAI_Gym\\.venv\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:310\u001B[0m, in \u001B[0;36m_create_concrete_function\u001B[1;34m(function_type, type_context, func_graph, tracing_options)\u001B[0m\n\u001B[0;32m    303\u001B[0m   placeholder_bound_args \u001B[38;5;241m=\u001B[39m function_type\u001B[38;5;241m.\u001B[39mplaceholder_arguments(\n\u001B[0;32m    304\u001B[0m       placeholder_context\n\u001B[0;32m    305\u001B[0m   )\n\u001B[0;32m    307\u001B[0m disable_acd \u001B[38;5;241m=\u001B[39m tracing_options\u001B[38;5;241m.\u001B[39mattributes \u001B[38;5;129;01mand\u001B[39;00m tracing_options\u001B[38;5;241m.\u001B[39mattributes\u001B[38;5;241m.\u001B[39mget(\n\u001B[0;32m    308\u001B[0m     attributes_lib\u001B[38;5;241m.\u001B[39mDISABLE_ACD, \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    309\u001B[0m )\n\u001B[1;32m--> 310\u001B[0m traced_func_graph \u001B[38;5;241m=\u001B[39m \u001B[43mfunc_graph_module\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfunc_graph_from_py_func\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    311\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtracing_options\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    312\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtracing_options\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpython_function\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    313\u001B[0m \u001B[43m    \u001B[49m\u001B[43mplaceholder_bound_args\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    314\u001B[0m \u001B[43m    \u001B[49m\u001B[43mplaceholder_bound_args\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    315\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    316\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfunc_graph\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfunc_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    317\u001B[0m \u001B[43m    \u001B[49m\u001B[43madd_control_dependencies\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdisable_acd\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    318\u001B[0m \u001B[43m    \u001B[49m\u001B[43marg_names\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfunction_type_utils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_arg_names\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfunction_type\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    319\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_placeholders\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    320\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    322\u001B[0m transform\u001B[38;5;241m.\u001B[39mapply_func_graph_transforms(traced_func_graph)\n\u001B[0;32m    324\u001B[0m graph_capture_container \u001B[38;5;241m=\u001B[39m traced_func_graph\u001B[38;5;241m.\u001B[39mfunction_captures\n",
      "File \u001B[1;32m~\\PycharmProjects\\Reinforcement_Learning_using_OpenAI_Gym\\.venv\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:987\u001B[0m, in \u001B[0;36mfunc_graph_from_py_func\u001B[1;34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001B[0m\n\u001B[0;32m    984\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    985\u001B[0m   deps_control_manager \u001B[38;5;241m=\u001B[39m ops\u001B[38;5;241m.\u001B[39mNullContextmanager()\n\u001B[1;32m--> 987\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m func_graph\u001B[38;5;241m.\u001B[39mas_default(), deps_control_manager \u001B[38;5;28;01mas\u001B[39;00m deps_ctx:\n\u001B[0;32m    988\u001B[0m   current_scope \u001B[38;5;241m=\u001B[39m variable_scope\u001B[38;5;241m.\u001B[39mget_variable_scope()\n\u001B[0;32m    989\u001B[0m   default_use_resource \u001B[38;5;241m=\u001B[39m current_scope\u001B[38;5;241m.\u001B[39muse_resource\n",
      "File \u001B[1;32m~\\PycharmProjects\\Reinforcement_Learning_using_OpenAI_Gym\\.venv\\lib\\site-packages\\tensorflow\\python\\framework\\auto_control_deps.py:533\u001B[0m, in \u001B[0;36mAutomaticControlDependencies.__exit__\u001B[1;34m(self, unused_type, unused_value, unused_traceback)\u001B[0m\n\u001B[0;32m    526\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m r\u001B[38;5;241m.\u001B[39mgraph\u001B[38;5;241m.\u001B[39mbuilding_function:\n\u001B[0;32m    527\u001B[0m   \u001B[38;5;66;03m# There may be many stateful ops in the graph. Adding them as\u001B[39;00m\n\u001B[0;32m    528\u001B[0m   \u001B[38;5;66;03m# control inputs to each function output could create excessive\u001B[39;00m\n\u001B[0;32m    529\u001B[0m   \u001B[38;5;66;03m# control edges in the graph. Thus we create an intermediate No-op to\u001B[39;00m\n\u001B[0;32m    530\u001B[0m   \u001B[38;5;66;03m# chain the control dependencies between stateful ops and function\u001B[39;00m\n\u001B[0;32m    531\u001B[0m   \u001B[38;5;66;03m# outputs.\u001B[39;00m\n\u001B[0;32m    532\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m idx \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m--> 533\u001B[0m     control_output_op \u001B[38;5;241m=\u001B[39m \u001B[43mcontrol_flow_ops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mno_op\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    534\u001B[0m     control_output_op\u001B[38;5;241m.\u001B[39m_add_control_inputs(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mops_which_must_run)\n\u001B[0;32m    535\u001B[0m   updated_ops_which_must_run \u001B[38;5;241m=\u001B[39m [control_output_op]\n",
      "File \u001B[1;32m~\\PycharmProjects\\Reinforcement_Learning_using_OpenAI_Gym\\.venv\\lib\\site-packages\\tensorflow\\python\\ops\\gen_control_flow_ops.py:531\u001B[0m, in \u001B[0;36mno_op\u001B[1;34m(name)\u001B[0m\n\u001B[0;32m    529\u001B[0m \u001B[38;5;66;03m# Add nodes to the TensorFlow graph.\u001B[39;00m\n\u001B[0;32m    530\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 531\u001B[0m   _, _, _op, _outputs \u001B[38;5;241m=\u001B[39m \u001B[43m_op_def_library\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply_op_helper\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    532\u001B[0m \u001B[43m      \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mNoOp\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    533\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mTypeError\u001B[39;00m, \u001B[38;5;167;01mValueError\u001B[39;00m):\n\u001B[0;32m    534\u001B[0m   _result \u001B[38;5;241m=\u001B[39m _dispatch\u001B[38;5;241m.\u001B[39mdispatch(\n\u001B[0;32m    535\u001B[0m         no_op, (), \u001B[38;5;28mdict\u001B[39m(name\u001B[38;5;241m=\u001B[39mname)\n\u001B[0;32m    536\u001B[0m       )\n",
      "File \u001B[1;32m~\\PycharmProjects\\Reinforcement_Learning_using_OpenAI_Gym\\.venv\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:755\u001B[0m, in \u001B[0;36m_apply_op_helper\u001B[1;34m(op_type_name, name, **keywords)\u001B[0m\n\u001B[0;32m    752\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_apply_op_helper\u001B[39m(op_type_name, name\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkeywords):  \u001B[38;5;66;03m# pylint: disable=invalid-name\u001B[39;00m\n\u001B[0;32m    753\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"Implementation of apply_op that returns output_structure, op.\"\"\"\u001B[39;00m\n\u001B[1;32m--> 755\u001B[0m   op_def, g, producer \u001B[38;5;241m=\u001B[39m \u001B[43m_GetOpDef\u001B[49m\u001B[43m(\u001B[49m\u001B[43mop_type_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkeywords\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    756\u001B[0m   name \u001B[38;5;241m=\u001B[39m name \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;28;01melse\u001B[39;00m op_type_name\n\u001B[0;32m    758\u001B[0m   attrs, attr_protos \u001B[38;5;241m=\u001B[39m {}, {}\n",
      "File \u001B[1;32m~\\PycharmProjects\\Reinforcement_Learning_using_OpenAI_Gym\\.venv\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:735\u001B[0m, in \u001B[0;36m_GetOpDef\u001B[1;34m(op_type_name, keywords)\u001B[0m\n\u001B[0;32m    731\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    732\u001B[0m   \u001B[38;5;66;03m# Need to flatten all the arguments into a list.\u001B[39;00m\n\u001B[0;32m    733\u001B[0m   \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[0;32m    734\u001B[0m   g \u001B[38;5;241m=\u001B[39m ops\u001B[38;5;241m.\u001B[39m_get_graph_from_inputs(_Flatten(keywords\u001B[38;5;241m.\u001B[39mvalues()))\n\u001B[1;32m--> 735\u001B[0m   producer \u001B[38;5;241m=\u001B[39m \u001B[43mg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgraph_def_versions\u001B[49m\u001B[38;5;241m.\u001B[39mproducer\n\u001B[0;32m    736\u001B[0m   \u001B[38;5;66;03m# pylint: enable=protected-access\u001B[39;00m\n\u001B[0;32m    737\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32m~\\PycharmProjects\\Reinforcement_Learning_using_OpenAI_Gym\\.venv\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:2278\u001B[0m, in \u001B[0;36mGraph.graph_def_versions\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   2267\u001B[0m \u001B[38;5;129m@property\u001B[39m\n\u001B[0;32m   2268\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgraph_def_versions\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m versions_pb2\u001B[38;5;241m.\u001B[39mVersionDef:\n\u001B[0;32m   2269\u001B[0m   \u001B[38;5;66;03m# pylint: disable=line-too-long\u001B[39;00m\n\u001B[0;32m   2270\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"The GraphDef version information of this graph.\u001B[39;00m\n\u001B[0;32m   2271\u001B[0m \n\u001B[0;32m   2272\u001B[0m \u001B[38;5;124;03m  For details on the meaning of each version, see\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   2276\u001B[0m \u001B[38;5;124;03m    A `VersionDef`.\u001B[39;00m\n\u001B[0;32m   2277\u001B[0m \u001B[38;5;124;03m  \"\"\"\u001B[39;00m\n\u001B[1;32m-> 2278\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mversions_pb2\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mVersionDef\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mFromString\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_version_def\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e326debda99ded58"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
